{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Sentimental_LIAR_RESNET.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1WJCWwHEKGjjS5gABAhF0pWdoeEUs1lvy",
      "authorship_tag": "ABX9TyNao8aZGEDf5tcA+0gxUFGH",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/10Exahertz/Text-ResNet-on-Sentimental-LIAR-Fake-News/blob/main/Sentimental_LIAR_RESNET.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1UFScqUBzH_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPYlAiZlGHjx"
      },
      "source": [
        "\n",
        "This is the code for the paper: https://github.com/10Exahertz/Text-ResNet-on-Sentimental-LIAR-Fake-News\n",
        "\n",
        "Sentimental LIAR: Extended Corpus with ResNet Architecture\n",
        "\n",
        "The code is for the best performing model where the Statement and Sentiment is passed into ELECTRA-base and the output of the ELECTRA-base is concatenated with Emotion Scores and then passed to the CNN. The model achieved the accuracy of 67.8% with a standard deviation of 1.41% and F1 Macro Score of 62.77% with a standard deviation of 1.41%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qhy5BD7fGQPY",
        "outputId": "e8040ae7-2f04-4655-de34-d89eb3f83217"
      },
      "source": [
        "!pip install -q transformers\n",
        "!pip install sentencepiece\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.95)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBbgMqYwGche",
        "outputId": "0b81a505-aa89-4b6c-b3a4-67f6bbbf7094"
      },
      "source": [
        "\n",
        "import pandas as pd\n",
        "df=pd.read_csv(\"/content/drive/MyDrive/Bibek/LIAR-DATASET ORIGINAL/final/train_final.csv\")\n",
        "df_test=pd.read_csv(\"/content/drive/MyDrive/Bibek/LIAR-DATASET ORIGINAL/final/test_final.csv\")\n",
        "df_valid=pd.read_csv(\"/content/drive/MyDrive/Bibek/LIAR-DATASET ORIGINAL/final/valid_final.csv\")\n",
        "print(\"before truncating size of data is :\", df.shape, df_test.shape,df_valid.shape)\n",
        "df=df[:10232] #10208\n",
        "df_test=df_test[:1264] #1248\n",
        "df_valid=df_valid[:1280] #1280\n",
        "print(\"size of data is :\", df.shape, df_test.shape, df_valid.shape)\n",
        "\n",
        "\n",
        "#check if any null values are present\n",
        "print(\"Any null in Subject? \",df['subject'].isnull().values.any())\n",
        "print(\"Any null in Speaker? \",df['speaker'].isnull().values.any())\n",
        "print(\"Any null in speaker_job? \",df['speaker_job'].isnull().values.any())\n",
        "print(\"Any null in Party? \",df['party_affiliation'].isnull().values.any())\n",
        "print(\"Any null in Context? \",df['context'].isnull().values.any())\n",
        "\n",
        "\n",
        "def listV2Out(inputLabel):\n",
        "  if inputLabel == 'pants-fire':\n",
        "    return '[1,0,0,0,0]'\n",
        "  if inputLabel == 'false':\n",
        "    return '[0,1,0,0,0]'\n",
        "  if inputLabel == 'barely-true':\n",
        "    return '[0,0,1,0,0]'\n",
        "  if inputLabel == 'half-true':\n",
        "    return '[0,0,0,1,0]'\n",
        "  if inputLabel == 'mostly-true':\n",
        "    return '[0,0,0,0,1]'\n",
        "  if inputLabel == 'true':\n",
        "    return '[0,0,0,0,1]'\n",
        "pd.set_option('display.max_columns', None)\n",
        "df['listV2'] = 0\n",
        "for i in range(len(df)):\n",
        "  df.loc[i,'listV2'] = listV2Out(df.loc[i,'label'])\n",
        "df_test['listV2'] = 0\n",
        "for i in range(len(df_test)):\n",
        "  df_test.loc[i,'listV2'] = listV2Out(df_test.loc[i,'label'])\n",
        "df_valid['listV2'] = 0\n",
        "for i in range(len(df_valid)):\n",
        "  df_valid.loc[i,'listV2'] = listV2Out(df_valid.loc[i,'label'])\n",
        "\n",
        "print(\"Any null in ListV2? \",df['listV2'].isnull().values.any())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "before truncating size of data is : (10236, 31) (1267, 30) (1283, 31)\n",
            "size of data is : (10232, 31) (1264, 30) (1280, 31)\n",
            "Any null in Subject?  False\n",
            "Any null in Speaker?  False\n",
            "Any null in speaker_job?  True\n",
            "Any null in Party?  False\n",
            "Any null in Context?  True\n",
            "Any null in ListV2?  False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyAjCea6HKuG"
      },
      "source": [
        "\n",
        "df['comment_text']=\"\"\n",
        "df_test['comment_text']=\"\"\n",
        "df_valid[\"comment_text\"]=\"\"\n",
        "\n",
        "#combine all the columns into one for train, test and valid data\n",
        "\n",
        "# df['comment_text']=df['subject'].astype(str)+ \". \"+ df[\"statement\"]+\\\n",
        "# df['speaker_id'].astype(str)+\". \"+df['speaker_job'].astype(str)+\". \"+ df['party_affiliation'].astype(str)\\\n",
        "# +\". \"+df['context'].astype(str)+\". \"+df['sentiment_code'].astype(str)\n",
        "df['comment_text']= df[\"statement\"]#+\". \"+df['sentiment_code'].astype(str)\n",
        "\n",
        "# df_test['comment_text']=df_test['subject'].astype(str)+ \". \"+ df_test[\"statement\"]+\". \"+\\\n",
        "# df_test['speaker_id'].astype(str)+\". \"+df_test['speaker_job'].astype(str)+\". \"+ df_test['party_affiliation'].astype(str)\\\n",
        "# +\". \"+df_test['context'].astype(str)+\". \"+df['sentiment_code'].astype(str)\n",
        "df_test['comment_text']= df_test[\"statement\"]#+\". \"+df_test['sentiment_code'].astype(str)\n",
        "\n",
        "# df_valid['comment_text']=df_valid['subject'].astype(str)+ \". \"+ df_valid[\"statement\"]+\\\n",
        "# df_valid['speaker_id'].astype(str)+\". \"+df_valid['speaker_job'].astype(str)+\". \"+ df_valid['party_affiliation'].astype(str)\\\n",
        "# +\". \"+df_valid['context'].astype(str)+\". \"+df_valid['sentiment_code'].astype(str)\n",
        "df_valid['comment_text']= df_valid[\"statement\"]#+\". \"+df_valid['sentiment_code'].astype(str)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVYgK5mGHomd"
      },
      "source": [
        "\n",
        "\n",
        "#concatenate emotion, speakers' credit and sentiment score togehter\n",
        "df['emotion']=\"[\"+df['anger'].astype(str)+\",\"+df['disgust'].astype(str)+\",\"\\\n",
        "+df['fear'].astype(str)+\",\"+df['joy'].astype(str)+\",\"+df['sad'].astype(str) + \",\" + df['sentiment_score'].astype(str)+\"]\" #+\",\"+\\\n",
        "# df[\"barely_true_counts\"].astype(str) +\",\"+ df[\"false_counts\"].astype(str)  +\",\"+\\\n",
        "# df[\"half_true_counts\"].astype(str) + \",\"+df[\"mostly_true_counts\"].astype(str) +\",\"+ \\\n",
        "# df[\"pants_on_fire_counts\"].astype(str)+\",\"+df[\"sentiment_score\"].astype(str)+\"]\"\n",
        "\n",
        "\n",
        "df_test['emotion']=\"[\"+df_test['anger'].astype(str)+\",\"+df_test['disgust'].astype(str)+\",\"\\\n",
        "+df_test['fear'].astype(str)+\",\"+df_test['joy'].astype(str)+\",\"+df_test['sad'].astype(str) + \",\" + df_test['sentiment_score'].astype(str)+\"]\"\n",
        "# + \",\"+df_test[\"barely_true_counts\"].astype(str) + \",\"+ df_test[\"false_counts\"].astype(str) \\\n",
        "# +\",\"+ df_test[\"half_true_counts\"].astype(str) +\",\"+ df_test[\"mostly_true_counts\"].astype(str)\\\n",
        "# +\",\"+ df_test[\"pants_on_fire_counts\"].astype(str)+\",\"+df_test[\"sentiment_score\"].astype(str)+\"]\"\n",
        "\n",
        "\n",
        "df_valid['emotion']=\"[\"+df_valid['anger'].astype(str)+\",\"+df_valid['disgust'].astype(str)+\",\"\\\n",
        "+df_valid['fear'].astype(str)+\",\"+df_valid['joy'].astype(str)+\",\"+df_valid['sad'].astype(str)+ \",\" + df_valid['sentiment_score'].astype(str)+\"]\"\n",
        "# + \",\"+df_valid[\"barely_true_counts\"].astype(str) + \",\"+ df_valid[\"false_counts\"].astype(str) \\\n",
        "# +\",\"+ df_valid[\"half_true_counts\"].astype(str) +\",\"+ df_valid[\"mostly_true_counts\"].astype(str)\\\n",
        "# +\",\"+ df_valid[\"pants_on_fire_counts\"].astype(str)+\",\"+df_valid[\"sentiment_score\"].astype(str)+\"]\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "om2o14P3Huin"
      },
      "source": [
        "#target should be converted from string to the list\n",
        "import ast\n",
        "def convert_to_list(text):\n",
        "  return ast.literal_eval(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2puH4ObHxaH",
        "outputId": "2be5b0ee-534b-46e5-fecc-f34b523b753d"
      },
      "source": [
        "for i in range(len(df[\"emotion\"])):\n",
        "  try:\n",
        "    df[\"emotion\"][i]=convert_to_list(df[\"emotion\"][i])\n",
        "  except:\n",
        "    print(i,\"====\",df[\"emotion\"][1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5YLrYzKHz9n",
        "outputId": "3b3c6efe-aa23-49f1-8487-2050f6b9f717"
      },
      "source": [
        "for i in range(len(df_valid[\"emotion\"])):\n",
        "  try:\n",
        "    df_valid[\"emotion\"][i]=convert_to_list(df_valid[\"emotion\"][i])\n",
        "  except:\n",
        "    print(i,\"====\",df_valid[\"emotion\"][i], type(df_valid[\"emotion\"][i]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6-j3_TvegZs",
        "outputId": "2b4099bb-3866-4e65-ccc3-c88022c42110"
      },
      "source": [
        "for i in range(len(df_test[\"emotion\"])):\n",
        "  try:\n",
        "    df_test[\"emotion\"][i]=convert_to_list(df_test[\"emotion\"][i])\n",
        "  except:\n",
        "    print(i,\"====\",df_test[\"emotion\"][i], type(df_test[\"emotion\"][i]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccwgxJyXH4FP"
      },
      "source": [
        "\n",
        "df['list']=df['list'].apply(convert_to_list)\n",
        "df_test['list']=df_test['list'].apply(convert_to_list)\n",
        "df_valid['list']=df_valid['list'].apply(convert_to_list)\n",
        "\n",
        "df['listV2']=df['listV2'].apply(convert_to_list)\n",
        "df_test['listV2']=df_test['listV2'].apply(convert_to_list)\n",
        "df_valid['listV2']=df_valid['listV2'].apply(convert_to_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LNGG6T6Gu-f",
        "outputId": "71a75bc3-3253-4fbb-d968-1fdcfbb9bf5e"
      },
      "source": [
        "#RESNET CODE: Base Class by Frencesco Zuppechini, https://towardsdatascience.com/residual-network-implementing-resnet-a7da63c7b278\n",
        "#Code was adapted for text based inputs with ELECTRA Embeddings, some changes throughout the architecture were also made, stage sizes and initial CNN layer sizes\n",
        " \n",
        " \n",
        "from functools import partial\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "import transformers\n",
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertTokenizer, BertModel, BertConfig, ElectraConfig, ElectraModel, ElectraTokenizer\n",
        " \n",
        "from torch import cuda\n",
        " \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        " \n",
        "from dataclasses import dataclass\n",
        "from collections import OrderedDict\n",
        " \n",
        " \n",
        " \n",
        "class Conv1dAuto(nn.Conv1d):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.padding =  [self.kernel_size[0] // 2]#, self.kernel_size[0] // 2) # dynamic add padding based on the kernel_size\n",
        "Kernel_size_main = 3\n",
        "Gate_k_size = 15\n",
        "Pool_k_size = 3\n",
        " \n",
        " \n",
        "conv1x3 = partial(Conv1dAuto, kernel_size=Kernel_size_main, bias=False) \n",
        "#conv = conv1x3(in_channels=32, out_channels=64)\n",
        " \n",
        "def activation_func(activation):\n",
        "    return  nn.ModuleDict([\n",
        "        ['relu', nn.ReLU(inplace=True)],\n",
        "        ['leaky_relu', nn.LeakyReLU(negative_slope=0.01, inplace=True)],\n",
        "        ['selu', nn.SELU(inplace=True)],\n",
        "        ['none', nn.Identity()]\n",
        "    ])[activation]\n",
        " \n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, activation='relu'):\n",
        "        super().__init__()\n",
        "        self.in_channels, self.out_channels, self.activation = in_channels, out_channels, activation\n",
        "        self.blocks = nn.Identity()\n",
        "        self.activate = activation_func(activation)\n",
        "        self.shortcut = nn.Identity()   \n",
        "    \n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        if self.should_apply_shortcut: residual = self.shortcut(x)\n",
        "        x = self.blocks(x)\n",
        "        x += residual\n",
        "        x = self.activate(x)\n",
        "        return x\n",
        "    \n",
        "    @property\n",
        "    def should_apply_shortcut(self):\n",
        "        return self.in_channels != self.out_channels\n",
        " \n",
        "class ResNetResidualBlock(ResidualBlock):\n",
        "    def __init__(self, in_channels, out_channels, expansion=1, downsampling=1, conv=conv1x3, *args, **kwargs):\n",
        "        super().__init__(in_channels, out_channels, *args, **kwargs)\n",
        "        self.expansion, self.downsampling, self.conv = expansion, downsampling, conv\n",
        "        self.shortcut = nn.Sequential(\n",
        "            nn.Conv1d(self.in_channels, self.expanded_channels, kernel_size=1,\n",
        "                      stride=self.downsampling, bias=False),\n",
        "            nn.BatchNorm1d(self.expanded_channels)) if self.should_apply_shortcut else None\n",
        "        \n",
        "        \n",
        "    @property\n",
        "    def expanded_channels(self):\n",
        "        return self.out_channels * self.expansion\n",
        "    \n",
        "    @property\n",
        "    def should_apply_shortcut(self):\n",
        "        return self.in_channels != self.expanded_channels\n",
        " \n",
        "def conv_bn(in_channels, out_channels, conv, *args, **kwargs):\n",
        "    return nn.Sequential(conv(in_channels, out_channels, *args, **kwargs), nn.BatchNorm1d(out_channels))\n",
        " \n",
        " \n",
        "class ResNetBasicBlock(ResNetResidualBlock):\n",
        "    \"\"\"\n",
        "    Basic ResNet block composed by two layers of 3x3conv/batchnorm/activation\n",
        "    \"\"\"\n",
        "    expansion = 1\n",
        "    def __init__(self, in_channels, out_channels, *args, **kwargs):\n",
        "        super().__init__(in_channels, out_channels, *args, **kwargs)\n",
        "        self.blocks = nn.Sequential(\n",
        "            conv_bn(self.in_channels, self.out_channels, conv=self.conv, bias=False, stride=self.downsampling),\n",
        "            #nn.Dropout(0.2),\n",
        "            activation_func(self.activation),\n",
        "            conv_bn(self.out_channels, self.expanded_channels, conv=self.conv, bias=False),\n",
        "        )\n",
        " \n",
        "class ResNetBottleNeckBlock(ResNetResidualBlock):\n",
        "    expansion = 4\n",
        "    def __init__(self, in_channels, out_channels, *args, **kwargs):\n",
        "        super().__init__(in_channels, out_channels, expansion=4, *args, **kwargs)\n",
        "        self.blocks = nn.Sequential(\n",
        "           conv_bn(self.in_channels, self.out_channels, self.conv, kernel_size=1),\n",
        "             activation_func(self.activation),\n",
        "             conv_bn(self.out_channels, self.out_channels, self.conv, kernel_size=Kernel_size_main, stride=self.downsampling),\n",
        "             activation_func(self.activation),\n",
        "             conv_bn(self.out_channels, self.expanded_channels, self.conv, kernel_size=1),\n",
        "        )\n",
        " \n",
        "class ResNetLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    A ResNet layer composed by `n` blocks stacked one after the other\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, block=ResNetBasicBlock, n=1, *args, **kwargs):\n",
        "        super().__init__()\n",
        "        # 'We perform downsampling directly by convolutional layers that have a stride of 2.'\n",
        "        downsampling = 2 if in_channels != out_channels else 1\n",
        "        self.blocks = nn.Sequential(\n",
        "            block(in_channels , out_channels, *args, **kwargs, downsampling=downsampling),\n",
        "            *[block(out_channels * block.expansion, \n",
        "                    out_channels, downsampling=1, *args, **kwargs) for _ in range(n - 1)]\n",
        "        )\n",
        " \n",
        "    def forward(self, x):\n",
        "        x = self.blocks(x)\n",
        "        return x\n",
        " \n",
        " \n",
        "class ResNetEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    ResNet encoder composed by layers with increasing features.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels=3, blocks_sizes=[64, 128, 256, 512], deepths=[2,2,2,2], \n",
        "                 activation='relu', block=ResNetBasicBlock, *args, **kwargs):\n",
        "        super().__init__()\n",
        "        self.blocks_sizes = blocks_sizes\n",
        "        \n",
        "        self.gate = nn.Sequential(\n",
        "            nn.Conv1d(in_channels, self.blocks_sizes[0], kernel_size=Gate_k_size, stride=1, padding=3, bias=False),\n",
        "            nn.BatchNorm1d(self.blocks_sizes[0]),\n",
        "            activation_func(activation),\n",
        "            nn.AvgPool1d(kernel_size=Pool_k_size, stride=1, padding=1)\n",
        "        )\n",
        "        \n",
        "        self.in_out_block_sizes = list(zip(blocks_sizes, blocks_sizes[1:]))\n",
        "        self.blocks = nn.ModuleList([ \n",
        "            ResNetLayer(blocks_sizes[0], blocks_sizes[0], n=deepths[0], activation=activation, \n",
        "                        block=block,*args, **kwargs),\n",
        "            *[ResNetLayer(in_channels * block.expansion, \n",
        "                          out_channels, n=n, activation=activation, \n",
        "                          block=block, *args, **kwargs) \n",
        "              for (in_channels, out_channels), n in zip(self.in_out_block_sizes, deepths[1:])]       \n",
        "        ])\n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.gate(x)\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        return x\n",
        " \n",
        "class ResnetDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    This class represents the tail of ResNet. It performs a global pooling and maps the output to the\n",
        "    correct class by using a fully connected layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, n_classes):\n",
        "        super().__init__()\n",
        "        self.maxx = nn.AdaptiveAvgPool1d(1)\n",
        "        self.decoder = nn.Linear(in_features, n_classes)\n",
        " \n",
        "    def forward(self, x):\n",
        "        x = self.maxx(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        " \n",
        " \n",
        "class ResNet(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_channels, n_classes, *args, **kwargs):\n",
        "        super().__init__()\n",
        "        self.encoder = ResNetEncoder(in_channels, *args, **kwargs)\n",
        "        self.decoder = ResnetDecoder(self.encoder.blocks[-1].blocks[-1].expanded_channels, n_classes)\n",
        "        #self.l1 = transformers.BertModel.from_pretrained('bert-base-uncased', return_dict=False)\n",
        "        self.l1 = transformers.ElectraModel.from_pretrained('google/electra-base-discriminator', return_dict = False)\n",
        "        self.l2 = torch.nn.Dropout(0.2)\n",
        "        \n",
        "    def forward(self, ids, mask, token_type_ids,emotion):\n",
        "        #_, output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids)\n",
        "        output_1 = self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids)[0]\n",
        "        output_1 = torch.stack([torch.mean(x,0) for x in output_1])\n",
        "        output_1 = self.l2(output_1)\n",
        "\n",
        "        output_1=torch.cat((emotion,output_1),1)\n",
        "        #print(output_1.size())\n",
        "        output_1=output_1.unsqueeze(1)\n",
        "        x = self.encoder(output_1)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        " \n",
        "def resnet10_3Stage(in_channels, n_classes, block=ResNetBasicBlock, *args, **kwargs):\n",
        "    return ResNet(in_channels, n_classes, block=block, deepths=[2,1,1], *args, **kwargs)\n",
        "\n",
        "def resnet10_2Stage(in_channels, n_classes, block=ResNetBasicBlock, *args, **kwargs):\n",
        "    return ResNet(in_channels, n_classes, block=block, deepths=[3,1], *args, **kwargs)\n",
        "\n",
        "def resnet10_1Stage(in_channels, n_classes, block=ResNetBasicBlock, *args, **kwargs):\n",
        "    return ResNet(in_channels, n_classes, block=block, deepths=[4], *args, **kwargs)\n",
        "\n",
        "def resnet6(in_channels, n_classes, block=ResNetBasicBlock, *args, **kwargs):\n",
        "    return ResNet(in_channels, n_classes, block=block, deepths=[1,1], *args, **kwargs)\n",
        " \n",
        "def resnet8(in_channels, n_classes, block=ResNetBasicBlock, *args, **kwargs):\n",
        "    return ResNet(in_channels, n_classes, block=block, deepths=[1, 1, 1], *args, **kwargs)\n",
        "\n",
        "def resnet8_2Stage(in_channels, n_classes, block=ResNetBasicBlock, *args, **kwargs):\n",
        "    return ResNet(in_channels, n_classes, block=block, deepths=[2,1], *args, **kwargs)\n",
        "\n",
        "def resnet8_1Stage(in_channels, n_classes, block=ResNetBasicBlock, *args, **kwargs):\n",
        "    return ResNet(in_channels, n_classes, block=block, deepths=[3], *args, **kwargs)\n",
        "\n",
        "def resnet10(in_channels, n_classes, block=ResNetBasicBlock, *args, **kwargs):\n",
        "    return ResNet(in_channels, n_classes, block=block, deepths=[1, 1, 1, 1], *args, **kwargs)\n",
        " \n",
        "def resnet12(in_channels, n_classes, block=ResNetBasicBlock, *args, **kwargs):\n",
        "    return ResNet(in_channels, n_classes, block=block, deepths=[2, 1, 1, 1], *args, **kwargs)\n",
        " \n",
        "def resnet18(in_channels, n_classes, block=ResNetBasicBlock, *args, **kwargs):\n",
        "    return ResNet(in_channels, n_classes, block=block, deepths=[2, 2, 2, 2], *args, **kwargs)\n",
        " \n",
        "def resnet34(in_channels, n_classes, block=ResNetBasicBlock, *args, **kwargs):\n",
        "    return ResNet(in_channels, n_classes, block=block, deepths=[3, 4, 6, 3], *args, **kwargs)\n",
        " \n",
        "def resnet50(in_channels, n_classes, block=ResNetBottleNeckBlock, *args, **kwargs):\n",
        "    return ResNet(in_channels, n_classes, block=block, deepths=[3, 4, 6, 3], *args, **kwargs)\n",
        "#from torchinfo import summary\n",
        " \n",
        "model = resnet10_3Stage(1, 2)\n",
        "#summary(model.cuda(), [(1,300),(1,300),(1,300),(1,11)])\n",
        "sum([param.nelement() for param in model.parameters()])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at google/electra-base-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight']\n",
            "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "109354818"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5-U_POpH62f"
      },
      "source": [
        " \n",
        " \n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        " \n",
        "MAX_LEN = 300\n",
        "TRAIN_BATCH_SIZE = 8\n",
        "VALID_BATCH_SIZE = 8\n",
        "EPOCHS = 2\n",
        "LEARNING_RATE = 1e-05\n",
        "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "tokenizer = ElectraTokenizer.from_pretrained('google/electra-base-discriminator')\n",
        " \n",
        "#tokenizer.pad_token = tokenizer.eos_token\n",
        " \n",
        " \n",
        " \n",
        "#torch.manual_seed(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aibi_b-l_0kz"
      },
      "source": [
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.comment_text = dataframe.comment_text\n",
        "        self.targets = dataframe.list\n",
        "        self.max_len = max_len\n",
        "        #Add emotion list from dataframe\n",
        "        self.emotion=dataframe.emotion\n",
        "        self.dfID=dataframe.ID\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.comment_text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        comment_text = str(self.comment_text[index])\n",
        "        comment_text = \" \".join(comment_text.split())\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            comment_text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "        token_type_ids = inputs[\"token_type_ids\"]\n",
        "\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            'targets': torch.tensor(self.targets[index], dtype=torch.float),\n",
        "            'emotion':torch.tensor(self.emotion[index], dtype=torch.float),\n",
        "            'dfID':self.dfID[index]\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yv7M41mwIdF8",
        "outputId": "d15813ef-574c-4062-8652-aa9f1d5cad32"
      },
      "source": [
        "# Creating the dataset and dataloader for the neural network\n",
        "\n",
        "train_size = 1\n",
        "train_dataset=df.sample(frac=train_size,random_state=200).reset_index(drop=True)\n",
        "test_dataset=df_test.sample(frac=train_size,random_state=200).reset_index(drop=True)\n",
        "valid_dataset=df_valid.sample(frac=1,random_state=200).reset_index(drop=True)\n",
        "\n",
        "\n",
        "print(\"FULL Dataset: {}\".format(df.shape))\n",
        "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
        "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
        "print(\"VALID Dataset: {}\".format(valid_dataset.shape))\n",
        "\n",
        "training_set = CustomDataset(train_dataset, tokenizer, MAX_LEN)\n",
        "for i in range(len(training_set)):\n",
        "    sample = training_set[i]\n",
        "    print(i, sample['ids'].size(), sample['mask'].size(), sample['token_type_ids'].size(), sample['targets'].size(), sample['emotion'].size(), sample['dfID'])\n",
        "\n",
        "    if i == 3:\n",
        "        break\n",
        "testing_set = CustomDataset(test_dataset, tokenizer, MAX_LEN)\n",
        "valid_set= CustomDataset(valid_dataset, tokenizer, MAX_LEN)\n",
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "print(type(df.loc[0,'emotion']))\n",
        "print(type(df_test.loc[0,'emotion']))\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "testing_loader = DataLoader(testing_set, **test_params)\n",
        "valid_loader=DataLoader(valid_set,**test_params)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FULL Dataset: (10232, 34)\n",
            "TRAIN Dataset: (10232, 34)\n",
            "TEST Dataset: (1264, 33)\n",
            "VALID Dataset: (1280, 34)\n",
            "0 torch.Size([300]) torch.Size([300]) torch.Size([300]) torch.Size([2]) torch.Size([6]) 6564.json\n",
            "1 torch.Size([300]) torch.Size([300]) torch.Size([300]) torch.Size([2]) torch.Size([6]) 7056.json\n",
            "2 torch.Size([300]) torch.Size([300]) torch.Size([300]) torch.Size([2]) torch.Size([6]) 11586.json\n",
            "3 torch.Size([300]) torch.Size([300]) torch.Size([300]) torch.Size([2]) torch.Size([6]) 9707.json\n",
            "<class 'list'>\n",
            "<class 'list'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lRIcBqjIj8j",
        "outputId": "33c07817-960f-4bdb-c39c-a12ebd1c162a"
      },
      "source": [
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (encoder): ResNetEncoder(\n",
              "    (gate): Sequential(\n",
              "      (0): Conv1d(1, 64, kernel_size=(15,), stride=(1,), padding=(3,), bias=False)\n",
              "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): AvgPool1d(kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "    )\n",
              "    (blocks): ModuleList(\n",
              "      (0): ResNetLayer(\n",
              "        (blocks): Sequential(\n",
              "          (0): ResNetBasicBlock(\n",
              "            (blocks): Sequential(\n",
              "              (0): Sequential(\n",
              "                (0): Conv1dAuto(64, 64, kernel_size=(3,), stride=(1,), padding=[1], bias=False)\n",
              "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "              (1): ReLU(inplace=True)\n",
              "              (2): Sequential(\n",
              "                (0): Conv1dAuto(64, 64, kernel_size=(3,), stride=(1,), padding=[1], bias=False)\n",
              "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "            )\n",
              "            (activate): ReLU(inplace=True)\n",
              "            (shortcut): None\n",
              "          )\n",
              "          (1): ResNetBasicBlock(\n",
              "            (blocks): Sequential(\n",
              "              (0): Sequential(\n",
              "                (0): Conv1dAuto(64, 64, kernel_size=(3,), stride=(1,), padding=[1], bias=False)\n",
              "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "              (1): ReLU(inplace=True)\n",
              "              (2): Sequential(\n",
              "                (0): Conv1dAuto(64, 64, kernel_size=(3,), stride=(1,), padding=[1], bias=False)\n",
              "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "            )\n",
              "            (activate): ReLU(inplace=True)\n",
              "            (shortcut): None\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1): ResNetLayer(\n",
              "        (blocks): Sequential(\n",
              "          (0): ResNetBasicBlock(\n",
              "            (blocks): Sequential(\n",
              "              (0): Sequential(\n",
              "                (0): Conv1dAuto(64, 128, kernel_size=(3,), stride=(2,), padding=[1], bias=False)\n",
              "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "              (1): ReLU(inplace=True)\n",
              "              (2): Sequential(\n",
              "                (0): Conv1dAuto(128, 128, kernel_size=(3,), stride=(1,), padding=[1], bias=False)\n",
              "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "            )\n",
              "            (activate): ReLU(inplace=True)\n",
              "            (shortcut): Sequential(\n",
              "              (0): Conv1d(64, 128, kernel_size=(1,), stride=(2,), bias=False)\n",
              "              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (2): ResNetLayer(\n",
              "        (blocks): Sequential(\n",
              "          (0): ResNetBasicBlock(\n",
              "            (blocks): Sequential(\n",
              "              (0): Sequential(\n",
              "                (0): Conv1dAuto(128, 256, kernel_size=(3,), stride=(2,), padding=[1], bias=False)\n",
              "                (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "              (1): ReLU(inplace=True)\n",
              "              (2): Sequential(\n",
              "                (0): Conv1dAuto(256, 256, kernel_size=(3,), stride=(1,), padding=[1], bias=False)\n",
              "                (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "            )\n",
              "            (activate): ReLU(inplace=True)\n",
              "            (shortcut): Sequential(\n",
              "              (0): Conv1d(128, 256, kernel_size=(1,), stride=(2,), bias=False)\n",
              "              (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (decoder): ResnetDecoder(\n",
              "    (maxx): AdaptiveAvgPool1d(output_size=1)\n",
              "    (decoder): Linear(in_features=256, out_features=2, bias=True)\n",
              "  )\n",
              "  (l1): ElectraModel(\n",
              "    (embeddings): ElectraEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): ElectraEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): ElectraLayer(\n",
              "          (attention): ElectraAttention(\n",
              "            (self): ElectraSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): ElectraSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ElectraIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): ElectraOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): ElectraLayer(\n",
              "          (attention): ElectraAttention(\n",
              "            (self): ElectraSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): ElectraSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ElectraIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): ElectraOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): ElectraLayer(\n",
              "          (attention): ElectraAttention(\n",
              "            (self): ElectraSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): ElectraSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ElectraIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): ElectraOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): ElectraLayer(\n",
              "          (attention): ElectraAttention(\n",
              "            (self): ElectraSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): ElectraSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ElectraIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): ElectraOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): ElectraLayer(\n",
              "          (attention): ElectraAttention(\n",
              "            (self): ElectraSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): ElectraSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ElectraIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): ElectraOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): ElectraLayer(\n",
              "          (attention): ElectraAttention(\n",
              "            (self): ElectraSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): ElectraSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ElectraIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): ElectraOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): ElectraLayer(\n",
              "          (attention): ElectraAttention(\n",
              "            (self): ElectraSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): ElectraSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ElectraIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): ElectraOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): ElectraLayer(\n",
              "          (attention): ElectraAttention(\n",
              "            (self): ElectraSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): ElectraSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ElectraIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): ElectraOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): ElectraLayer(\n",
              "          (attention): ElectraAttention(\n",
              "            (self): ElectraSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): ElectraSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ElectraIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): ElectraOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): ElectraLayer(\n",
              "          (attention): ElectraAttention(\n",
              "            (self): ElectraSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): ElectraSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ElectraIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): ElectraOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): ElectraLayer(\n",
              "          (attention): ElectraAttention(\n",
              "            (self): ElectraSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): ElectraSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ElectraIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): ElectraOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): ElectraLayer(\n",
              "          (attention): ElectraAttention(\n",
              "            (self): ElectraSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): ElectraSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ElectraIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): ElectraOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (l2): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1yAfDGeImz2"
      },
      "source": [
        " \n",
        "import time\n",
        "import datetime\n",
        "def format_time(elapsed):\n",
        "    elapsed_rounded = int(round((elapsed)))    \n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkZBgF_rIqC7"
      },
      "source": [
        "def loss_fn(outputs, targets):\n",
        "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)\n",
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
        "#optimizer = torch.optim.SGD(params = model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
        "#optimizer = torch.optim.AdamW(params = model.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpnOijRvIsF0",
        "outputId": "d83f0ec0-10b1-4d6a-ad7d-8e6b3b297c30"
      },
      "source": [
        "#TRAIN THE MODEL\n",
        "val_losses=[]\n",
        "train_losses=[]\n",
        "accuracy_list=[]\n",
        " \n",
        "for epoch in range(EPOCHS):\n",
        "  t0 = time.time()\n",
        "  model.train()\n",
        "  print(f\"\\t Epoch: {epoch}  is Started: \")\n",
        "  batch=0\n",
        "  train_loss=0\n",
        "  \n",
        "  for _,data in enumerate(training_loader, 0):\n",
        "      try:\n",
        "          ids = data['ids'].to(device, dtype = torch.long)\n",
        "          mask = data['mask'].to(device, dtype = torch.long)\n",
        "          token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "          targets = data['targets'].to(device, dtype = torch.float)\n",
        "          emotions=data['emotion'].to(device,dtype=torch.float)\n",
        "      except:\n",
        "          print(f\"some error at testing {batch}\")\n",
        "          print(data['dfID'] )\n",
        "      try:  \n",
        "        outputs = model(ids, mask, token_type_ids,emotions)\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        train_loss+=loss.item()\n",
        "        #print(f'{count} Loss:  {loss.item()}')\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        batch+=1\n",
        "      except EOFError:\n",
        "        print(f\"{data['dfID']} error because of batch size-------->\", EOFError)\n",
        "        print(f\"some error at testing {batch}\")\n",
        "        print(data['dfID'] )\n",
        "  print(f\"   Epoch: {epoch} Train loss is :{train_loss/batch}\") \n",
        "  train_loss /=batch\n",
        "  train_losses.append(train_loss)    \n",
        "  print(f\"   Epoch {epoch} took: {format_time(time.time() - t0)} \\n\")\n",
        " \n",
        "  model.eval()\n",
        "  fin_targets=[]\n",
        "  fin_outputs=[]\n",
        "  with torch.no_grad():\n",
        "      val_loss, batch = 0, 1\n",
        "      for _, data in enumerate(testing_loader, 0):\n",
        "          ids = data['ids'].to(device, dtype = torch.long)\n",
        "          mask = data['mask'].to(device, dtype = torch.long)\n",
        "          token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "          targets = data['targets'].to(device, dtype = torch.float)\n",
        "          emotions=data['emotion'].to(device,dtype=torch.float)\n",
        "          batch+=1\n",
        "          try:\n",
        "                        outputs = model(ids, mask, token_type_ids,emotions)\n",
        "                        loss = loss_fn(outputs, targets)\n",
        "                        val_loss+=loss.item()\n",
        "                        fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
        "                        fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
        "          except:\n",
        "            print(f\"some error at testing {batch}\")\n",
        "            print(data['dfID'] )\n",
        " \n",
        "      val_loss/=batch\n",
        "      val_losses.append(val_loss)\n",
        "  outputs=fin_outputs\n",
        "  outputs = np.array(outputs) >= 0.5\n",
        "  targets=fin_targets\n",
        "  accuracy = metrics.accuracy_score(targets, outputs)\n",
        "  accuracy_list.append(accuracy)\n",
        "  f1_score_micro = metrics.f1_score(targets, outputs, average='micro')\n",
        "  f1_score_macro = metrics.f1_score(targets, outputs, average='macro')\n",
        "  print(f\"Epoch: {epoch} - Accuracy on Testing Data Score = {accuracy}\")\n",
        "  print(f\"Epoch: {epoch} - F1 Score on Testing Data (Micro) = {f1_score_micro}\")\n",
        "  print(f\"Epoch: {epoch} - F1 Score on Testing Data (Macro) = {f1_score_macro}\")\n",
        "  print(f\"\\n \\t Epoch {epoch} : Train Loss (Training Data):{train_loss}, Validation Loss (Testing Data): {val_loss}\")\n",
        "  print(\"_________________________________________________\\n\")\n",
        "  #if train_loss > val_loss:\n",
        "  # torch.save(model.state_dict(), \"/content/drive/My Drive/Bibek/models_saved/w9p7\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t Epoch: 0  is Started: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "   Epoch: 0 Train loss is :0.634795990350938\n",
            "   Epoch 0 took: 0:03:29 \n",
            "\n",
            "Epoch: 0 - Accuracy on Testing Data Score = 0.6400316455696202\n",
            "Epoch: 0 - F1 Score on Testing Data (Micro) = 0.6511627906976745\n",
            "Epoch: 0 - F1 Score on Testing Data (Macro) = 0.6322659084223943\n",
            "\n",
            " \t Epoch 0 : Train Loss (Training Data):0.634795990350938, Validation Loss (Testing Data): 0.6196078054560056\n",
            "_________________________________________________\n",
            "\n",
            "\t Epoch: 1  is Started: \n",
            "   Epoch: 1 Train loss is :0.5906110264017211\n",
            "   Epoch 1 took: 0:03:29 \n",
            "\n",
            "Epoch: 1 - Accuracy on Testing Data Score = 0.6613924050632911\n",
            "Epoch: 1 - F1 Score on Testing Data (Micro) = 0.6650868878357029\n",
            "Epoch: 1 - F1 Score on Testing Data (Macro) = 0.6337414214558211\n",
            "\n",
            " \t Epoch 1 : Train Loss (Training Data):0.5906110264017211, Validation Loss (Testing Data): 0.5934983098282004\n",
            "_________________________________________________\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WC85FQv2Ix-6"
      },
      "source": [
        "from pandas import DataFrame\n",
        "df=DataFrame(train_losses,columns=['train_losses'])\n",
        "df=DataFrame(val_losses,columns=['val_losses'])\n",
        "df.to_csv(\"/content/drive/My Drive/Bibek/Results/w10_p4.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DYYauxJI3NY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "377b2793-aab9-4d22-96e7-7e8310979dfb"
      },
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(train_losses, label=\"Training loss\")\n",
        "plt.plot(val_losses, label=\"Validation loss\")\n",
        "\n",
        "plt.xlabel('Epoch', fontsize=18)\n",
        "plt.ylabel('Losses', fontsize=16)\n",
        "plt.title('Training Loss VS Validation Loss', fontsize=15)\n",
        "\n",
        "plt.legend()\n",
        "plt.savefig('/content/drive/My Drive/Bibek/w10_p4-epoch1.eps')\n",
        "\n",
        "#plt.title(\"Losses\")\n",
        "#ResNet10, 5 epochs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
            "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSJFohS5I6eu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18d3989c-8f75-4d8f-cfe4-916888aca5e8"
      },
      "source": [
        "model.eval()\n",
        "fin_targets=[]\n",
        "fin_outputs=[]\n",
        "with torch.no_grad():\n",
        "    c=0\n",
        "    for _, data in enumerate(valid_loader, 0):\n",
        "        ids = data['ids'].to(device, dtype = torch.long)\n",
        "        mask = data['mask'].to(device, dtype = torch.long)\n",
        "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "        targets = data['targets'].to(device, dtype = torch.float)\n",
        "        emotions=data['emotion'].to(device,dtype=torch.float)\n",
        "        c+=1\n",
        "        try:\n",
        "                      outputs = model(ids, mask, token_type_ids,emotions)\n",
        "                      fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
        "                      fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
        "        except EOFError:\n",
        "          print(f\"some error at {c}\",EOFError)\n",
        "outputs=fin_outputs\n",
        "outputs = np.array(outputs) >= 0.5\n",
        "targets=fin_targets\n",
        "accuracy = metrics.accuracy_score(targets, outputs)\n",
        " \n",
        "f1_score_macro = metrics.f1_score(targets, outputs, average='macro')\n",
        "print(f\"Epoch:  {epoch}, Accuracy Score on validation data = {accuracy}\")\n",
        " \n",
        "print(f\"Epoch:  {epoch}, F1 Score on Validation Data (Macro) = {f1_score_macro}\") \n",
        "print(\"____________________________________________________________\\n________________________________________________________________\\n\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:  1, Accuracy Score on validation data = 0.6796875\n",
            "Epoch:  1, F1 Score on Validation Data (Macro) = 0.6324548825710754\n",
            "____________________________________________________________\n",
            "________________________________________________________________\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKX2jVownUng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "653deeae-17ce-486b-b0e1-0b905aefdb23"
      },
      "source": [
        "df_demo=pd.read_csv(\"/content/drive/MyDrive/Bibek/LIAR-DATASET ORIGINAL/final/Demo.csv\")\n",
        "df_demo['comment_text']=\"\"\n",
        "df_demo['comment_text']= df_demo[\"statement\"]+\". \"+df_demo['sentiment_code'].astype(str)\n",
        "\n",
        "df_demo['emotion']=\"[\"+df_demo['anger'].astype(str)+\",\"+df_demo['disgust'].astype(str)+\",\"\\\n",
        "+df_demo['fear'].astype(str)+\",\"+df_demo['joy'].astype(str)+\",\"+df_demo['sad'].astype(str) + \"]\"\n",
        "for i in range(len(df_demo[\"emotion\"])):\n",
        "  try:\n",
        "    df_demo[\"emotion\"][i]=convert_to_list(df_demo[\"emotion\"][i])\n",
        "  except:\n",
        "    print(i,\"====\",df_demo[\"emotion\"][1])\n",
        "\n",
        "df_demo['list']=df_demo['list'].apply(convert_to_list)\n",
        "\n",
        "\n",
        "demo_set = CustomDataset(df_demo, tokenizer, MAX_LEN)\n",
        "\n",
        "demo_params = {'batch_size': 8,\n",
        "                'shuffle': False,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "demo_loader = DataLoader(demo_set, **demo_params)\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    c=0\n",
        "    for _, data in enumerate(demo_loader, 0):\n",
        "        ids = data['ids'].to(device, dtype = torch.long)\n",
        "        mask = data['mask'].to(device, dtype = torch.long)\n",
        "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "        targets = data['targets'].to(device, dtype = torch.float)\n",
        "        emotions=data['emotion'].to(device,dtype=torch.float)\n",
        "        c+=1\n",
        "        try:\n",
        "                      outputs = model(ids, mask, token_type_ids,emotions)\n",
        "                      fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
        "                      fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
        "                      print(torch.sigmoid(outputs))\n",
        "                      # print(emotions)\n",
        "\n",
        "\n",
        "        except EOFError:\n",
        "          print(f\"some error at {c}\",EOFError)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.3571, 0.6562],\n",
            "        [0.3571, 0.6562],\n",
            "        [0.4887, 0.5302],\n",
            "        [0.4402, 0.5721],\n",
            "        [0.2100, 0.8025],\n",
            "        [0.2806, 0.7300],\n",
            "        [0.3925, 0.6204],\n",
            "        [0.1871, 0.8221]], device='cuda:0')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bU3b4Mxt3P4Y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}