# Applying Deeper Convolutional Networks on The Sentimental LIAR Dataset


# Abstract 
We take the Sentimental LIAR dataset and network and perform network analysis and hyper-parametertuning on the original network.   We also apply the ResNet architecture on the network for deeperlearning. We find the ELECTRA embeddings are most well suited for the fake news classification task.A statistically significant F1 score increase of 0.061 was achieved. Both 4 Layer CNN and ResNet10architecture reach a similar performance level potentially demonstrating a larger dataset is needed forbetter performance

# Introduction

The maturation of the internet saw it take on a new and dangerous role in the modern political landscape. The last yearhas demonstrated the dangers of this new medium and the radical political hack-packs that it nurtures. Since the start ofthe Covid-19 pandemic we have seen one wave of misinformation after the other. Early on it was the 5G stories, and agovernment operated "plandemic". Then came the Bill Gate’s Vaccination chips over the summer, as the then Presidentmask usage misinformation. We saw the meteoric rise of Q-Anon, a group of rabid conspiracy theorists whose root ideologyis similar to the Nazi Conspiracies. This Q-Anon group grew in power rapidly, and in January 2021 a coalition of groupsinvolved in Q-Anon in some way or another stormed the US Capitol believing the 2020 Election lie that Donald Trump hadwon, purporting to "Stop the Steal." In the past it led to the rash Brexit decision, and coordinated propaganda campaigns onFacebook are considered to have greatly contributed to inciting the Myanmar Genocides. Fake News is a palpable issuethat must be resolved in some way.The body of work on fake news detection on long form documents, ones that come with a large body of context are growingrapidly. However, short text fake news detection systems are more rare, they are just as important considering the commonlyused social media sites use short form text. These shorter form texts provide the obvious issue of not enough context andinformation leaving the models trained on longer form fake news detection tasks constrained. More recently some datasetsand networks have been proposed to resolve these issues. The LIAR dataset contains short form texts from Politifact.com,a fact checking website. The dataset was a first of its kind and is the largest one for short form fake news classification.Sentimental LIAR took it a step further, adding in sentiment scores and emotion scores and made significant performancebosts over the original LIAR paper. The rest of this paper will include using deeper learning frame works from the field ofcomputer vision, namely the ResNet network and larger CNN networks to see if improvements can be made on the currentnetwork. The idea being that deeper, more hierarchical learning without vanishing gradients that ResNets provide can findbetter features and boost the performance. We also provide analysis on the model structure and input structure.

# Methods   
## Convolutional Neural Network
The general idea of the LeNet-5 like CNN for classification purposes is to take the input data and apply a convolvingkernel/filter over the text. In images a convolution allows for smoothing, and edge detection, depending on the kernel type.In a Convolutional Neural Net the kernel’s functions are learned, allowing it to do far more complex tasks. The deeper thenetwork, and the more channels, the better the feature representation is and the better the performance. until a cliff wherethe data is insufficient in some way, or vanishing gradients come into play.2
arXivTemplateThe Sentimental LIAR paper used a 2D CNN with max pooling layers in between. Below is a diagram showing the generalnetwork that we will be using for our testing. The one shown below is the 4 Layer CNN. We start with the input into theEmbedding System (ELECTRA-base here), this includes the Statement and the Binary Sentiment. The dimensions of allembedding systems is constrained to a size of 768, larger embeddings performed worse.  Then we add on the emotionscores before pushing it through the CNN layers. Each CNN layer uses a max-pooling, we use a [50,100,200,400] channelexpansion schema in this paper. We then feed it into a fully connected linear layer before passing it on to the classifier.

![alt text](Images/CNN4Text.jpg?raw=true)

## ResNet for Text Classification

ResNet Figure B is the image of a basic residual block.  The curving line shows the skip-connection or residual connectionthat allows the block, the two 3x3 convolutional layers in this diagram, to learn the residual instead of the final output.This allows a bypass of the vanishing gradient problem with deep neural networks. There are no gates in a ResNet skipconnection, and in a way this allows for memory to be passed along the network. It does this by taking the output of oneblock and adding it to the output of the next block. Passing it along down the network. This has allowed for very deep CNN’s to be created for very complex tasks.

![alt text](Images/ResNet10TextandBlock

Above is the architecture of the ResNet10 Text Classification Network. Each block contains two 1x3 convolutional layers.We start with the Statement and Sentiment and move them through an embedding generator as before. We then append theemotion scores to this output. We first run a 1x15 CNN layer over this input, this essentially becomes a look up table forthe rest of the ResNet. This ResNet10 network contains only 3 Expansion Stages. Each stage in a ResNet Network expandsthe number of dimensions. The normal expansion schema is [64, 128, 256, 512] for a 4 stage ResNet. After going throughthe 3 stages with residual connections, we arrive at a fully connected linear layer and a classifier. Every layer uses AveragePooling, ReLU activation functions and the first convolutional layer uses dropout for stability. In experiments we will detailmore model types, and of different sizes, but ResNet10 with 3 Stages is our primary ResNet model.

# Experiments
We wanted a model that was as powerful as possible but also as usable as possible.  The Politifact.com set-up for eachquestion would constrain a network to only work on that website.  We want a model that has more general uses.  As aresult we also focused on generalization. In the same branch of logic we want to make sure the model is actually using thestatements and its embeddings to make decisions and want to understand where the model is getting performance boostsfrom. Below is a number of experiments that seek to answer all of these questions and cover all of these topics.
## Sentimental LIAR vs LIAR
The original LIAR paper contained 5 classes ranging from "Pants on fire" to "Mostly True" as stated previously.  We wanted to run the 2 Layer CNN network (Original LIAR paper did not specify these parameters) and test the original LIAR paper for a more direct comparison. We also were curious how much a boost the Sentimental LIAR paper received bythe reduction of the number for classes from 5 to 2, the binary True and False. The Table below shows the results of this experiment. 

![alt text](Images/TableEx1.jpg)

It is clear that the Sentimental LIAR network, that is including the sentiments and emotion scores gets a boost on themore important F1 score (imbalanced dataset), but that the overall performance of the network is granted via the binaryclassification system. These models were only run once.

## Generalization
Going through the total architecture of the Sentimental LIAR network we noticed a few issues. One was that the use ofSpeaker name, job, party affiliation, and so would not generalize well outside of Politifact. So we ran an experiment to seehow much of a performance change would come from removing the speaker information. We also noticed an odd addition.On Politifact.com a team of journalists votes on the Truthfulness of the statement, this creates a vote count for the six LIARclasses. Reading the journalist vote counts makes it incredible easy to distinguish truth and fact, this should in no waybe included in the network. Since all this will do is set the network up to look for these counts to make an easy decision,this is useless.  Taking the Speaker info out we see a small increase in the F1 score.  Likely due to making informationmore salient, less info means its easier to distinguish important features elsewhere. We notice without the vote counts theperformance on the F1 score drops 5 points to 0.5646. Standard Deviations are shown next to the results below.

![alt text](Images/TableExOriginal.jpg)

## Different Embeddings
As stated before RoBERTa, ALBERT, XLNet, GPT-2, and ELECTRA are some of these new, state of the art models.RoBERTa and ALBERT seek to resolve the next-sentence prediction task and increase training size and performance.ALBERT is state of the art on semantic text tasks such as STS and SST tasks.  ELECTRA changes the generator to adiscriminator. ELECTRA has state of the art performances on question and reading comprehension tasks. Below shows theresults of all of these models. This experiment was run before the vote counts were taken out, changes should be the samesince these apply to statement embeddings. For ELECTRA and non-sentence embeddings systems we used an averagingmethod for sequence embeddings.

![](Images/TableEmbeddings.jpg)

